{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PREREQUISITE -\n",
    "\n",
    "Install the below Environments\n",
    "\n",
    "        -pip install gym==0.10.5\n",
    "        -pip install pybullet==2.0.8\n",
    "        -conda install -c conda-forge ffmpeg\n",
    "\n",
    "\n",
    "The Half-Cheetah Environment\n",
    "We will play with the Half-Cheetah environment, inside which we will train with our ARS a\n",
    "cheetah to run across a field. The name of this environment is “HalfCheetahBulletEnv-v0”.\n",
    "The environments of PyBullet belong to the “pybullet_envs” module, so we have to include the\n",
    "following import at the beginning of our Python implementation (in the “Importing the libraries”\n",
    "code section):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# AI 2018\n",
    "\n",
    "# Importing the libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import pybullet_envs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Setting the Hyper Parameters\n",
    "\n",
    "class Hp():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nb_steps = 150\n",
    "        self.episode_length = 1000\n",
    "        self.learning_rate = 0.02\n",
    "        self.nb_directions = 16\n",
    "        self.nb_best_directions = 16\n",
    "        assert self.nb_best_directions <= self.nb_directions\n",
    "        self.noise = 0.03\n",
    "        self.seed = 1\n",
    "        self.env_name = 'HalfCheetahBulletEnv-v0'\n",
    "\n",
    "# Normalizing the states\n",
    "\n",
    "class Normalizer():\n",
    "    \n",
    "    def __init__(self, nb_inputs):\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "    \n",
    "    def observe(self, x):\n",
    "        self.n += 1.\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "    \n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - obs_mean) / obs_std\n",
    "\n",
    "# Building the AI\n",
    "\n",
    "class Policy():\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.theta = np.zeros((output_size, input_size))\n",
    "    \n",
    "    def evaluate(self, input, delta = None, direction = None):\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input)\n",
    "        elif direction == \"positive\":\n",
    "            return (self.theta + hp.noise*delta).dot(input)\n",
    "        else:\n",
    "            return (self.theta - hp.noise*delta).dot(input)\n",
    "    \n",
    "    def sample_deltas(self):\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(hp.nb_directions)]\n",
    "    \n",
    "    def update(self, rollouts, sigma_r):\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, d in rollouts:\n",
    "            step += (r_pos - r_neg) * d\n",
    "        self.theta += hp.learning_rate / (hp.nb_best_directions * sigma_r) * step\n",
    "\n",
    "# Exploring the policy on one specific direction and over one episode\n",
    "\n",
    "def explore(env, normalizer, policy, direction = None, delta = None):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    num_plays = 0.\n",
    "    sum_rewards = 0\n",
    "    while not done and num_plays < hp.episode_length:\n",
    "        normalizer.observe(state)\n",
    "        state = normalizer.normalize(state)\n",
    "        action = policy.evaluate(state, delta, direction)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward = max(min(reward, 1), -1)\n",
    "        sum_rewards += reward\n",
    "        num_plays += 1\n",
    "    return sum_rewards\n",
    "\n",
    "# Training the AI\n",
    "\n",
    "def train(env, policy, normalizer, hp):\n",
    "    \n",
    "    for step in range(hp.nb_steps):\n",
    "        \n",
    "        # Initializing the perturbations deltas and the positive/negative rewards\n",
    "        deltas = policy.sample_deltas()\n",
    "        positive_rewards = [0] * hp.nb_directions\n",
    "        negative_rewards = [0] * hp.nb_directions\n",
    "        \n",
    "        # Getting the positive rewards in the positive directions\n",
    "        for k in range(hp.nb_directions):\n",
    "            positive_rewards[k] = explore(env, normalizer, policy, direction = \"positive\", delta = deltas[k])\n",
    "        \n",
    "        # Getting the negative rewards in the negative/opposite directions\n",
    "        for k in range(hp.nb_directions):\n",
    "            negative_rewards[k] = explore(env, normalizer, policy, direction = \"negative\", delta = deltas[k])\n",
    "        \n",
    "        # Gathering all the positive/negative rewards to compute the standard deviation of these rewards\n",
    "        all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "        sigma_r = all_rewards.std()\n",
    "        \n",
    "        # Sorting the rollouts by the max(r_pos, r_neg) and selecting the best directions\n",
    "        scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "        order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:hp.nb_best_directions]\n",
    "        rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "        \n",
    "        # Updating our policy\n",
    "        policy.update(rollouts, sigma_r)\n",
    "        \n",
    "        # Printing the final reward of the policy after the update\n",
    "        reward_evaluation = explore(env, normalizer, policy)\n",
    "        print('Step:', step, 'Reward:', reward_evaluation)\n",
    "\n",
    "# Running the main code\n",
    "\n",
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('exp2', 'brs2')\n",
    "monitor_dir = mkdir(work_dir, 'monitor1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\Arun\\\\Documents'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.chdir('F:\\\\ARS\\\\')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Setting the Hyper Parameters\n",
    "\n",
    "class Hp():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nb_steps = 200\n",
    "        self.episode_length = 1000\n",
    "        self.learning_rate = 0.02\n",
    "        self.nb_directions = 16\n",
    "        self.nb_best_directions = 16\n",
    "        assert self.nb_best_directions <= self.nb_directions\n",
    "        self.noise = 0.03\n",
    "        self.seed = 1\n",
    "        self.env_name = 'HalfCheetahBulletEnv-v0'\n",
    "\n",
    "# Normalizing the states\n",
    "\n",
    "class Normalizer():\n",
    "    \n",
    "    def __init__(self, nb_inputs):\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "    \n",
    "    def observe(self, x):\n",
    "        self.n += 1.\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "    \n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - obs_mean) / obs_std\n",
    "\n",
    "# Building the AI\n",
    "\n",
    "class Policy():\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self.theta = np.zeros((output_size, input_size))\n",
    "    \n",
    "    def evaluate(self, input, delta = None, direction = None):\n",
    "        if direction is None:\n",
    "            return self.theta.dot(input)\n",
    "        elif direction == \"positive\":\n",
    "            return (self.theta + hp.noise*delta).dot(input)\n",
    "        else:\n",
    "            return (self.theta - hp.noise*delta).dot(input)\n",
    "    \n",
    "    def sample_deltas(self):\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(hp.nb_directions)]\n",
    "    \n",
    "    def update(self, rollouts, sigma_r):\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, d in rollouts:\n",
    "            step += (r_pos - r_neg) * d\n",
    "        self.theta += hp.learning_rate / (hp.nb_best_directions * sigma_r) * step\n",
    "\n",
    "# Exploring the policy on one specific direction and over one episode\n",
    "\n",
    "def explore(env, normalizer, policy, direction = None, delta = None):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    num_plays = 0.\n",
    "    sum_rewards = 0\n",
    "    while not done and num_plays < hp.episode_length:\n",
    "        normalizer.observe(state)\n",
    "        state = normalizer.normalize(state)\n",
    "        action = policy.evaluate(state, delta, direction)\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        reward = max(min(reward, 1), -1)\n",
    "        sum_rewards += reward\n",
    "        num_plays += 1\n",
    "    return sum_rewards\n",
    "\n",
    "# Training the AI\n",
    "\n",
    "def train(env, policy, normalizer, hp):\n",
    "    \n",
    "    for step in range(hp.nb_steps):\n",
    "        \n",
    "        # Initializing the perturbations deltas and the positive/negative rewards\n",
    "        deltas = policy.sample_deltas()\n",
    "        positive_rewards = [0] * hp.nb_directions\n",
    "        negative_rewards = [0] * hp.nb_directions\n",
    "        \n",
    "        # Getting the positive rewards in the positive directions\n",
    "        for k in range(hp.nb_directions):\n",
    "            positive_rewards[k] = explore(env, normalizer, policy, direction = \"positive\", delta = deltas[k])\n",
    "        \n",
    "        # Getting the negative rewards in the negative/opposite directions\n",
    "        for k in range(hp.nb_directions):\n",
    "            negative_rewards[k] = explore(env, normalizer, policy, direction = \"negative\", delta = deltas[k])\n",
    "        \n",
    "        # Gathering all the positive/negative rewards to compute the standard deviation of these rewards\n",
    "        all_rewards = np.array(positive_rewards + negative_rewards)\n",
    "        sigma_r = all_rewards.std()\n",
    "        \n",
    "        # Sorting the rollouts by the max(r_pos, r_neg) and selecting the best directions\n",
    "        scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "        order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:hp.nb_best_directions]\n",
    "        rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "        \n",
    "        # Updating our policy\n",
    "        policy.update(rollouts, sigma_r)\n",
    "        \n",
    "        # Printing the final reward of the policy after the update\n",
    "        reward_evaluation = explore(env, normalizer, policy)\n",
    "        print('Step:', step, 'Reward:', reward_evaluation)\n",
    "\n",
    "# Running the main code\n",
    "\n",
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "work_dir = mkdir('train', 'ars')\n",
    "monitor_dir = mkdir(work_dir, 'steps_monitor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "WalkerBase::__init__ start\n",
      "\u001b[33mWARN: Environment '<class 'pybullet_envs.gym_locomotion_envs.HalfCheetahBulletEnv'>' has deprecated methods '_step' and '_reset' rather than 'step' and 'reset'. Compatibility code invoked. Set _gym_disable_underscore_compat = True to disable this behavior.\u001b[0m\n",
      "Step: 0 Reward: -934.7060945122172\n",
      "Step: 1 Reward: -971.4918744077402\n",
      "Step: 2 Reward: -967.9451679353086\n",
      "Step: 3 Reward: -965.0323979906696\n",
      "Step: 4 Reward: -965.5929339924869\n",
      "Step: 5 Reward: -964.2459440011208\n",
      "Step: 6 Reward: -970.286312755245\n",
      "Step: 7 Reward: -904.5808245893616\n",
      "Step: 8 Reward: -938.9110729966255\n",
      "Step: 9 Reward: -895.416908171691\n",
      "Step: 10 Reward: -939.9486300217067\n",
      "Step: 11 Reward: -944.3907255523602\n",
      "Step: 12 Reward: -325.9843118951843\n",
      "Step: 13 Reward: -925.756336765783\n",
      "Step: 14 Reward: -707.2427339797815\n",
      "Step: 15 Reward: -437.4396323294142\n",
      "Step: 16 Reward: -899.7301851712066\n",
      "Step: 17 Reward: -528.970642276198\n",
      "Step: 18 Reward: -927.1538185734189\n",
      "Step: 19 Reward: -917.9057215345941\n",
      "Step: 20 Reward: -382.74795968732644\n",
      "Step: 21 Reward: -290.0812625833259\n",
      "Step: 22 Reward: -539.3796735230434\n",
      "Step: 23 Reward: -176.684488334774\n",
      "Step: 24 Reward: -201.91496024259612\n",
      "Step: 25 Reward: -565.191634902191\n",
      "Step: 26 Reward: -872.2119851298683\n",
      "Step: 27 Reward: -163.279071412045\n",
      "Step: 28 Reward: -63.17513321509238\n",
      "Step: 29 Reward: -492.91915055473424\n",
      "Step: 30 Reward: -4.47026042785784\n",
      "Step: 31 Reward: -134.8278276389441\n",
      "Step: 32 Reward: -130.2894837368423\n",
      "Step: 33 Reward: -193.35844623245853\n",
      "Step: 34 Reward: -158.08696798841925\n",
      "Step: 35 Reward: -148.53168899963308\n",
      "Step: 36 Reward: -23.80788585430744\n",
      "Step: 37 Reward: -65.21021014701361\n",
      "Step: 38 Reward: -73.15825367133009\n",
      "Step: 39 Reward: 2.588864656585319\n",
      "Step: 40 Reward: 62.250944682835815\n",
      "Step: 41 Reward: 18.25568565581625\n",
      "Step: 42 Reward: -32.31946706315645\n",
      "Step: 43 Reward: 108.2421909690593\n",
      "Step: 44 Reward: -32.558236361323225\n",
      "Step: 45 Reward: -32.88594746115722\n",
      "Step: 46 Reward: -6.413614647671175\n",
      "Step: 47 Reward: 71.5124300532895\n",
      "Step: 48 Reward: 111.14805012708916\n",
      "Step: 49 Reward: 47.601070706051345\n",
      "Step: 50 Reward: 55.62558891405471\n",
      "Step: 51 Reward: 113.67743266105661\n",
      "Step: 52 Reward: 60.64020120107687\n",
      "Step: 53 Reward: 65.70588348683933\n",
      "Step: 54 Reward: 95.24583215670238\n",
      "Step: 55 Reward: 126.50771561717706\n",
      "Step: 56 Reward: 114.52871354722743\n",
      "Step: 57 Reward: 173.41910199072112\n",
      "Step: 58 Reward: 192.63952980317825\n",
      "Step: 59 Reward: 130.96078856415255\n",
      "Step: 60 Reward: 241.33340997466715\n",
      "Step: 61 Reward: 240.28102831696324\n",
      "Step: 62 Reward: 308.46732024668665\n",
      "Step: 63 Reward: 295.44870685444977\n",
      "Step: 64 Reward: 166.7786943956189\n",
      "Step: 65 Reward: 331.54256064281844\n",
      "Step: 66 Reward: 361.996159033006\n",
      "Step: 67 Reward: 388.8724489742536\n",
      "Step: 68 Reward: 378.5837524052955\n",
      "Step: 69 Reward: 317.7807757520299\n",
      "Step: 70 Reward: 283.2362867385437\n",
      "Step: 71 Reward: 403.6997009942426\n",
      "Step: 72 Reward: 392.44457958817696\n",
      "Step: 73 Reward: 310.5259348976923\n",
      "Step: 74 Reward: 377.8778802644943\n",
      "Step: 75 Reward: 400.73640887795233\n",
      "Step: 76 Reward: 424.65440765645496\n",
      "Step: 77 Reward: 444.0082868064824\n",
      "Step: 78 Reward: 430.89934926263425\n",
      "Step: 79 Reward: 471.2564777540518\n",
      "Step: 80 Reward: 489.6404246369859\n",
      "Step: 81 Reward: 517.1543218616207\n",
      "Step: 82 Reward: 531.2619761056801\n",
      "Step: 83 Reward: 525.7742274191746\n",
      "Step: 84 Reward: 543.1410979491533\n",
      "Step: 85 Reward: 578.1823997455991\n",
      "Step: 86 Reward: 554.5789071149179\n",
      "Step: 87 Reward: 579.9720164337331\n",
      "Step: 88 Reward: 583.7699106182163\n",
      "Step: 89 Reward: 522.5541855599051\n",
      "Step: 90 Reward: 582.498860279896\n",
      "Step: 91 Reward: 486.19087159121915\n",
      "Step: 92 Reward: 666.9193414051659\n",
      "Step: 93 Reward: 523.934655168775\n",
      "Step: 94 Reward: 563.6276456169824\n",
      "Step: 95 Reward: 605.1234786382868\n",
      "Step: 96 Reward: 603.0057078761951\n",
      "Step: 97 Reward: 570.2003773897693\n",
      "Step: 98 Reward: 666.9989019644864\n",
      "Step: 99 Reward: 665.9008928129994\n",
      "Step: 100 Reward: 622.0250379186247\n",
      "Step: 101 Reward: 601.4202740585916\n",
      "Step: 102 Reward: 683.0484668860345\n",
      "Step: 103 Reward: 650.3282289627203\n",
      "Step: 104 Reward: 631.283928904076\n",
      "Step: 105 Reward: 670.8453826319774\n",
      "Step: 106 Reward: 678.5929597223904\n",
      "Step: 107 Reward: 719.9917663986263\n",
      "Step: 108 Reward: 710.9691450779923\n",
      "Step: 109 Reward: 707.0310004808175\n",
      "Step: 110 Reward: 694.2366987154809\n",
      "Step: 111 Reward: 632.0129220017263\n",
      "Step: 112 Reward: 688.2824681189081\n",
      "Step: 113 Reward: 658.6056205380902\n",
      "Step: 114 Reward: 678.6628465162821\n",
      "Step: 115 Reward: 686.7979611470927\n",
      "Step: 116 Reward: 685.0144590219127\n",
      "Step: 117 Reward: 700.1254160156195\n",
      "Step: 118 Reward: 694.9548587474321\n",
      "Step: 119 Reward: 713.1641808423565\n",
      "Step: 120 Reward: 712.4979862452939\n",
      "Step: 121 Reward: 726.2574352940367\n",
      "Step: 122 Reward: 699.2454063111343\n",
      "Step: 123 Reward: 689.1975029160809\n",
      "Step: 124 Reward: 692.4022903740264\n",
      "Step: 125 Reward: 710.9388081946455\n",
      "Step: 126 Reward: 724.3401751940256\n",
      "Step: 127 Reward: 717.349048038744\n",
      "Step: 128 Reward: 727.8946484523436\n",
      "Step: 129 Reward: 699.1498613555913\n",
      "Step: 130 Reward: 722.6444318471631\n",
      "Step: 131 Reward: 696.4227872792306\n",
      "Step: 132 Reward: 700.5536220679687\n",
      "Step: 133 Reward: 702.5126616198169\n",
      "Step: 134 Reward: 664.604189063371\n",
      "Step: 135 Reward: 670.0781127108739\n",
      "Step: 136 Reward: 704.7640445994027\n",
      "Step: 137 Reward: 682.6621794663052\n",
      "Step: 138 Reward: 662.1834228997518\n",
      "Step: 139 Reward: 708.4227208006389\n",
      "Step: 140 Reward: 677.457262625472\n",
      "Step: 141 Reward: 668.3708984518642\n",
      "Step: 142 Reward: 691.3540384129024\n",
      "Step: 143 Reward: 659.4919996512108\n",
      "Step: 144 Reward: 727.9533747954112\n",
      "Step: 145 Reward: 722.3656711007773\n",
      "Step: 146 Reward: 722.131656533866\n",
      "Step: 147 Reward: 689.7909484017566\n",
      "Step: 148 Reward: 685.9871731295683\n",
      "Step: 149 Reward: 700.3988592690649\n",
      "Step: 150 Reward: 694.4201859078281\n",
      "Step: 151 Reward: 682.5517436712731\n",
      "Step: 152 Reward: 719.994089480343\n",
      "Step: 153 Reward: 752.6952408758713\n",
      "Step: 154 Reward: 727.8677497071069\n",
      "Step: 155 Reward: 717.7411677494187\n",
      "Step: 156 Reward: 729.1132764994384\n",
      "Step: 157 Reward: 719.2220251964392\n",
      "Step: 158 Reward: 728.3401115492327\n",
      "Step: 159 Reward: 730.3439806517853\n",
      "Step: 160 Reward: 760.0710990997275\n",
      "Step: 161 Reward: 743.7960030722925\n",
      "Step: 162 Reward: 737.6786172982249\n",
      "Step: 163 Reward: 725.7777592755428\n",
      "Step: 164 Reward: 728.1447025321968\n",
      "Step: 165 Reward: 718.8623210179755\n",
      "Step: 166 Reward: 725.7349840594032\n",
      "Step: 167 Reward: 721.5334881230315\n",
      "Step: 168 Reward: 708.0048220934351\n",
      "Step: 169 Reward: 730.0366395684824\n",
      "Step: 170 Reward: 734.2980216837774\n",
      "Step: 171 Reward: 714.8372301302879\n",
      "Step: 172 Reward: 720.2057649671168\n",
      "Step: 173 Reward: 722.6414897667375\n",
      "Step: 174 Reward: 726.7574523574982\n",
      "Step: 175 Reward: 672.8762537021811\n",
      "Step: 176 Reward: 711.3748226911154\n",
      "Step: 177 Reward: 702.6449485266222\n",
      "Step: 178 Reward: 728.1581584945006\n",
      "Step: 179 Reward: 717.2469747662419\n",
      "Step: 180 Reward: 744.8416143641134\n",
      "Step: 181 Reward: 755.7475360670292\n",
      "Step: 182 Reward: 729.1404375533659\n",
      "Step: 183 Reward: 739.9116192286431\n",
      "Step: 184 Reward: 722.8975239063456\n",
      "Step: 185 Reward: 741.6843266150595\n",
      "Step: 186 Reward: 746.0298629817054\n",
      "Step: 187 Reward: 748.9599821968898\n",
      "Step: 188 Reward: 747.5301542442609\n",
      "Step: 189 Reward: 708.198143069947\n",
      "Step: 190 Reward: 759.4202694201689\n",
      "Step: 191 Reward: 768.0732855859624\n",
      "Step: 192 Reward: 761.3935804859436\n",
      "Step: 193 Reward: 716.0431003650959\n",
      "Step: 194 Reward: 760.1600886149744\n",
      "Step: 195 Reward: 743.1591469673696\n",
      "Step: 196 Reward: 746.0876699269331\n",
      "Step: 197 Reward: 687.6245500427424\n",
      "Step: 198 Reward: 737.1926657472575\n",
      "Step: 199 Reward: 735.3754551245881\n"
     ]
    }
   ],
   "source": [
    "\n",
    "hp = Hp()\n",
    "np.random.seed(hp.seed)\n",
    "env = gym.make(hp.env_name)\n",
    "env = wrappers.Monitor(env, monitor_dir, force = True)\n",
    "nb_inputs = env.observation_space.shape[0]\n",
    "nb_outputs = env.action_space.shape[0]\n",
    "policy = Policy(nb_inputs, nb_outputs)\n",
    "normalizer = Normalizer(nb_inputs)\n",
    "train(env, policy, normalizer, hp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
